{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating and augmenting images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=180,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "baseurl = '/Users/ggarbagnati/ds/metis/metisgh/sf17_ds5/local/Projects/05-Kojak'\n",
    "url = baseurl + '/data/train/apple_healthy/Apple-healthy-01251.jpg'\n",
    "\n",
    "#print(url)\n",
    "\n",
    "img = load_img(url)  # this is a PIL image\n",
    "x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "# the .flow() command below generates batches of randomly transformed images\n",
    "# and saves the results to the `preview/` directory\n",
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=1,\n",
    "                          save_to_dir='preview', save_prefix='apple_healthy', save_format='jpeg'):\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        break  # otherwise the generator would loop indefinitely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2698 images belonging to 2 classes.\n",
      "Found 1154 images belonging to 2 classes.\n",
      "Epoch 1/2\n",
      "2698/2698 [==============================] - 359s - loss: 0.2525 - acc: 0.8892 - val_loss: 0.0938 - val_acc: 0.9480\n",
      "Epoch 2/2\n",
      "2698/2698 [==============================] - 249s - loss: 0.1390 - acc: 0.9411 - val_loss: 0.0947 - val_acc: 0.9662\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The code snippet below is our first model, a simple stack of 3 convolution layers \n",
    "with a ReLU activation and followed by max-pooling layers. This is very similar to \n",
    "the architectures that Yann LeCun advocated in the 1990s for image classification \n",
    "(with the exception of ReLU).\n",
    "'''\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "#from keras.backend import image_data_format\n",
    "from keras import backend as K\n",
    "\n",
    "#img_width, img_height = 150, 150\n",
    "img_width, img_height = 761, 800\n",
    "\n",
    "'''\n",
    "if image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "'''\n",
    "\n",
    "input_shape = (150, 150, 3)\n",
    "#input_shape = (3, 150, 150)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(Conv2D(32, (3, 3), input_shape=(3, 150, 150)))\n",
    "model.add(Conv2D(32, 3, 3, input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Conv2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Conv2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "'''\n",
    "On top of it we stick two fully-connected layers. We end the model with a single \n",
    "unit and a sigmoid activation, which is perfect for a binary classification. To \n",
    "go with it we will also use the binary_crossentropy loss to train our model.\n",
    "'''\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "# model.add(Activation('softmax')) # for multiclass\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# for multiclass\n",
    "'''\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "'''\n",
    "'''\n",
    "Let's prepare our data. We will use .flow_from_directory() to generate batches \n",
    "of image data (and their labels) directly from our jpgs in their respective \n",
    "folders.\n",
    "'''\n",
    " \n",
    "batch_size = 16\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolders of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "\n",
    "basedir = '/Users/ggarbagnati/ds/metis/metisgh/sf17_ds5/local/Projects/05-Kojak'\n",
    "targetdir = basedir + '/data/train'\n",
    "valdir = basedir + '/data/validation'\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        targetdir,  # this is the target directory (originally = 'data/train')\n",
    "        target_size=(150, 150),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        valdir, # (originally 'data/validation')\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "'''\n",
    "We can now use these generators to train our model. Each epoch takes 20-30s on \n",
    "GPU and 300-400s on CPU. So it's definitely viable to run this model on CPU if \n",
    "you aren't in a hurry.\n",
    "'''\n",
    "\n",
    "#epochs=1, # original 50, 400s/epoch on cpu (20s/epoch on gpu)\n",
    "'''\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000 // batch_size,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800 // batch_size)\n",
    "'''\n",
    "\n",
    "nb_epoch = 2\n",
    "nb_train_samples = 320 + 2378\n",
    "nb_validation_samples = 136 + 1018\n",
    "\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch = nb_train_samples,\n",
    "        nb_epoch = nb_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples = nb_validation_samples)\n",
    "model.save_weights('first_try.h5')  # always save your weights after training or during training\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gets jpg count for all images in given directory and all subdirectories\n",
    "def jpg_counts(dirpath):\n",
    "    from os import listdir, walk\n",
    "    #from os.path import isfile, join\n",
    "    \n",
    "    # list of all subdirectories\n",
    "    dirlist = [x[0] for x in walk(dirpath)][1:]\n",
    "    \n",
    "    # list of all images in this directory\n",
    "    imagelist = [f for f in listdir(dirpath) if '.jpg' in f[-4:].lower()]\n",
    "    \n",
    "    # get all images in all subdirectories\n",
    "    #print(dirlist)\n",
    "    for currdir in dirlist:\n",
    "        imagelist += [f for f in listdir(currdir) if '.jpg' in f[-4:].lower()]\n",
    "    \n",
    "    return len(imagelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiclass classifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4033 images belonging to 4 classes.\n",
      "Found 1725 images belonging to 4 classes.\n",
      "Epoch 1/1\n",
      "4033/4033 [==============================] - 369s - loss: 0.6575 - acc: 0.7605 - val_loss: 0.1891 - val_acc: 0.9299\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The code snippet below is our first model, a simple stack of 3 convolution layers \n",
    "with a ReLU activation and followed by max-pooling layers. This is very similar to \n",
    "the architectures that Yann LeCun advocated in the 1990s for image classification \n",
    "(with the exception of ReLU).\n",
    "'''\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "#from keras.backend import image_data_format\n",
    "from keras import backend as K\n",
    "\n",
    "from os import walk\n",
    "\n",
    "#img_width, img_height = 150, 150\n",
    "img_width, img_height = 761, 800\n",
    "\n",
    "input_shape = (150, 150, 3)\n",
    "#input_shape = (3, 150, 150)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(Conv2D(32, (3, 3), input_shape=(3, 150, 150)))\n",
    "model.add(Conv2D(32, 3, 3, input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Conv2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Conv2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "'''\n",
    "On top of it we stick two fully-connected layers. We end the model with a single \n",
    "unit and a sigmoid activation, which is perfect for a binary classification. To \n",
    "go with it we will also use the binary_crossentropy loss to train our model.\n",
    "'''\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(Dense(1))\n",
    "model.add(Dense(4))\n",
    "#model.add(Activation('sigmoid'))\n",
    "model.add(Activation('softmax')) # for multiclass\n",
    "\n",
    "'''\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "'''\n",
    "\n",
    "# for multiclass\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Let's prepare our data. We will use .flow_from_directory() to generate batches \n",
    "of image data (and their labels) directly from our jpgs in their respective \n",
    "folders.\n",
    "'''\n",
    " \n",
    "batch_size = 16\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolders of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "\n",
    "basedir = '/Users/ggarbagnati/ds/metis/metisgh/sf17_ds5/local/Projects/05-Kojak'\n",
    "targetdir = basedir + '/data/train'\n",
    "valdir = basedir + '/data/validation'\n",
    "\n",
    "classes = next(walk(targetdir))[1]\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        targetdir,  # this is the target directory (originally = 'data/train')\n",
    "        target_size=(150, 150),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        classes=classes)\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        valdir, # (originally 'data/validation')\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        classes=classes)\n",
    "\n",
    "'''\n",
    "We can now use these generators to train our model. Each epoch takes 20-30s on \n",
    "GPU and 300-400s on CPU. So it's definitely viable to run this model on CPU if \n",
    "you aren't in a hurry.\n",
    "'''\n",
    "\n",
    "#epochs=1, # original 50, 400s/epoch on cpu (20s/epoch on gpu)\n",
    "'''\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000 // batch_size,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800 // batch_size)\n",
    "'''\n",
    "\n",
    "nb_epoch = 1\n",
    "nb_train_samples = jpg_counts(targetdir)\n",
    "nb_validation_samples = jpg_counts(valdir)\n",
    "\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch = nb_train_samples,\n",
    "        nb_epoch = nb_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples = nb_validation_samples)\n",
    "model.save_weights('first_try.h5')  # always save your weights after training or during training\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/ggarbagnati/ds/metis/metisgh/sf17_ds5/local/Projects/05-Kojak/data/train/strawberry_healthy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ddbd368b82e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtargetdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbasedir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/data/train'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcurrDir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargetdir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/strawberry_healthy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mimagelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrDir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimagelist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/ggarbagnati/ds/metis/metisgh/sf17_ds5/local/Projects/05-Kojak/data/train/strawberry_healthy'"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "\n",
    "basedir = '/Users/ggarbagnati/ds/metis/metisgh/sf17_ds5/local/Projects/05-Kojak'\n",
    "#mypath = '/Users/ggarbagnati/ds/metis/metisgh/sf17_ds5/local/Projects/05-Kojak/image_csv/'\n",
    "targetdir = basedir + '/data/train'\n",
    "currDir = targetdir + '/strawberry_healthy'\n",
    "imagelist = listdir(currDir)\n",
    "test_size = int(len(imagelist))\n",
    "print(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gets jpg count for all images in given directory and all subdirectories\n",
    "def jpg_counts(dirpath):\n",
    "    from os import listdir, walk\n",
    "    #from os.path import isfile, join\n",
    "    \n",
    "    # list of all subdirectories\n",
    "    dirlist = [x[0] for x in walk(dirpath)][1:]\n",
    "    \n",
    "    # list of all images in this directory\n",
    "    imagelist = [f for f in listdir(dirpath) if '.jpg' in f[-4:].lower()]\n",
    "    \n",
    "    # get all images in all subdirectories\n",
    "    #print(dirlist)\n",
    "    for currdir in dirlist:\n",
    "        imagelist += [f for f in listdir(currdir) if '.jpg' in f[-4:].lower()]\n",
    "    \n",
    "    return len(imagelist)\n",
    "    \n",
    "    #filelist = listdir(currDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_num_classes(dirpath, verbose=False):\n",
    "    from os import walk\n",
    "    \n",
    "    # get list of all direct subdirectories\n",
    "    dirlist = next(walk(dirpath))[1]\n",
    "    \n",
    "    if verbose:\n",
    "        print('Classes found:')\n",
    "        for d in dirlist:\n",
    "            print(d)\n",
    "    \n",
    "    return len(dirlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['banana_Banana_speckle',\n",
       " 'banana_Black_sigatoka_(Black_leaf_streak)',\n",
       " 'banana_healthy']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basedir = '/Users/ggarbagnati/ds/metis/metisgh/sf17_ds5/local/Projects/05-Kojak'\n",
    "targetdir = basedir + '/data/train'\n",
    "valdir = basedir + '/data/validation'\n",
    "from os import walk\n",
    "listdir = next(walk(targetdir))[1]\n",
    "print(type(listdir))\n",
    "listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num classes 53\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "52283"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basedir = '/Users/ggarbagnati/ds/metis/metisgh/sf17_ds5/local/Projects/05-Kojak/data3/train'\n",
    "print('num classes', get_num_classes(basedir))\n",
    "jpg_counts(basedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple_Apple_scab\n",
      "apple_Black_rot\n",
      "apple_Cedar_apple_rust\n",
      "apple_healthy\n",
      "banana_Banana_speckle\n",
      "banana_Black_sigatoka_(Black_leaf_streak)\n",
      "banana_healthy\n",
      "cabbage_Black_rot\n",
      "cabbage_healthy\n",
      "cassava_Brown_leaf_spot\n",
      "cassava_Cassava_green_spider_mite\n",
      "cherry_healthy\n",
      "cherry_Powdery_mildew\n",
      "corn_Cercospora_leaf_spot_Gray_leaf_spot\n",
      "corn_Common_rust\n",
      "corn_healthy\n",
      "corn_Northern_Leaf_Blight\n",
      "cucumber_Downy_mildew\n",
      "cucumber_healthy\n",
      "grape_Black_rot\n",
      "grape_Esca_(Black_Measles_or_Spanish_Measles)\n",
      "grape_healthy\n",
      "grape_Leaf_blight_(Isariopsis_Leaf_Spot)\n",
      "peach_Bacterial_spot\n",
      "peach_healthy\n",
      "Pepper,_bell_Anthracnose\n",
      "Pepper,_bell_Bacterial_spot\n",
      "Pepper,_bell_Bacterial_wilt\n",
      "Pepper,_bell_Cercospora_leaf_spot_(Frogeye_leaf_spot)\n",
      "Pepper,_bell_healthy\n",
      "potato_Early_blight\n",
      "potato_healthy\n",
      "potato_Late_blight\n",
      "soybean_Downy_mildew\n",
      "soybean_Frogeye_leaf_spot\n",
      "soybean_healthy\n",
      "soybean_Septoria_Leaf_Blight\n",
      "squash_Alternaria_leaf_spot\n",
      "squash_healthy\n",
      "squash_Powdery_mildew\n",
      "strawberry_healthy\n",
      "strawberry_Leaf_scorch\n",
      "tomato_Bacterial_spot\n",
      "tomato_Early_blight\n",
      "tomato_healthy\n",
      "tomato_Late_blight\n",
      "tomato_Leaf_Mold\n",
      "tomato_Magnesium_deficiency\n",
      "tomato_Septoria_leaf_spot\n",
      "tomato_Spider_mites??Two-spotted_spider_mite\n",
      "tomato_Target_Spot\n",
      "tomato_Tomato_mosaic_virus\n",
      "tomato_Tomato_Yellow_Leaf_Curl_disease\n",
      "num classes 53\n",
      "train\n",
      "validation\n",
      "num classes 2\n"
     ]
    }
   ],
   "source": [
    "basedir = '/Users/ggarbagnati/ds/metis/metisgh/sf17_ds5/local/Projects/05-Kojak/data3/train'\n",
    "print('num classes', get_num_classes(basedir))\n",
    "basedir = '/Users/ggarbagnati/ds/metis/metisgh/sf17_ds5/local/Projects/05-Kojak/data3/'\n",
    "print('num classes', get_num_classes(basedir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ggarbagnati/ds/metis/metisgh/sf17_ds5/local/Projects/05-Kojak/data/train/banana_Banana_speckle 2299\n",
      "/Users/ggarbagnati/ds/metis/metisgh/sf17_ds5/local/Projects/05-Kojak/data/validation/banana_Banana_speckle 985\n",
      "/Users/ggarbagnati/ds/metis/metisgh/sf17_ds5/local/Projects/05-Kojak/data/train/banana_Black_sigatoka_(Black_leaf_streak) 168\n",
      "/Users/ggarbagnati/ds/metis/metisgh/sf17_ds5/local/Projects/05-Kojak/data/validation/banana_Black_sigatoka_(Black_leaf_streak) 72\n",
      "/Users/ggarbagnati/ds/metis/metisgh/sf17_ds5/local/Projects/05-Kojak/data/train/banana_healthy 1151\n",
      "/Users/ggarbagnati/ds/metis/metisgh/sf17_ds5/local/Projects/05-Kojak/data/validation/banana_healthy 492\n"
     ]
    }
   ],
   "source": [
    "for d in listdir:\n",
    "    currTdir = targetdir + '/' + d\n",
    "    currVdir = valdir + '/' + d\n",
    "    print(currTdir, jpg_counts(currTdir))\n",
    "    print(currVdir, jpg_counts(currVdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications import inception_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = inception_v3.InceptionV3(include_top = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x1137eba90>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x104d26438>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x115537908>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x1155a9550>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11570b518>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11576e6d8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11579d6a0>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x11593fcf8>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x1159d9d30>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1159fdb00>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x115a6fc50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x115aadf60>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x115afb240>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x115e85da0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x115ed07b8>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x115bcecc0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x1160a2a20>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x115c8f630>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11610fe48>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x11633dd30>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x115b81f98>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x115cff748>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x116149780>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11639d4a8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x115b9e828>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x115af2cc0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x116140978>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1163bbe10>,\n",
       " <keras.engine.topology.Merge at 0x116408e48>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x116775898>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1169341d0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x1164d2940>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x116953940>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1166806d8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x116b05f98>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x116c75668>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x116449f28>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x1166a7630>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x116b37dd8>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x116c010f0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1163f4c50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1166a01d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x116be55c0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x116c88cc0>,\n",
       " <keras.engine.topology.Merge at 0x116e0f780>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x117070940>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11721ff98>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x116f28358>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11724fdd8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x116f65b00>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1172fe5c0>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x117527828>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x116e65f60>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x116f930f0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11742eb00>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11757cda0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x116ea2f28>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x117041f98>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11741a0f0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11760eeb8>,\n",
       " <keras.engine.topology.Merge at 0x11762a908>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x117712940>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1177c3f98>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x117910dd8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1179c15c0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11763e518>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x117a52668>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1176d0a90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x117a95be0>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x117c0fda0>,\n",
       " <keras.engine.topology.Merge at 0x117aefcf8>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x117906e80>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1180d8cc0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11813ef98>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11819ae80>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x117ca2588>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x1181e6780>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x117d35b00>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11838a8d0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x117d8be48>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x1183dbbe0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x117e27be0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1183c66d8>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x1187778d0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x117c44e48>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x117e4d908>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11867c860>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x1187ecf60>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x117c7efd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x117efcf60>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1186a3940>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x118808668>,\n",
       " <keras.engine.topology.Merge at 0x1188a2358>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x118cf4e10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x118c98e80>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x118e92e48>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x118f26e10>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x118982ac8>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x118f7bda0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1189b4e48>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x118e47fd0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x1189f6e48>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x1191a5c18>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x118ba6898>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x119222ef0>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x11932f8d0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x1188d3fd0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x118bcc5c0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x119277c88>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x119361c50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1188c06a0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x118c70cc0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11925e240>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1193589b0>,\n",
       " <keras.engine.topology.Merge at 0x119440da0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x119ab6860>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x119b0ed68>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x119b74f28>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x119be5cc0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x1195ae320>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x119d15940>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11977df60>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x119dc2898>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x1197ebf98>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x119de95c0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11995be48>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x119f8fc50>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x11a0c87b8>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x119563ef0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11998ee10>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11a014da0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11a096390>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x119575c50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x119a3bbe0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11a066f28>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11a257710>,\n",
       " <keras.engine.topology.Merge at 0x11a2c7e10>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11a802cc0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11a878fd0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11a8a68d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11a8cf9b0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11a47add8>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11a9a28d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11a4ffcf8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11ab36dd8>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11a57ce80>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11ab8bd68>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11a527eb8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11abbaf60>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x11ae57630>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11a26ae80>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11a62f828>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11ad41588>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11ae0eda0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11a423e48>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11a6b69b0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11adc1eb8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11aebc748>,\n",
       " <keras.engine.topology.Merge at 0x11af59390>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11b2dfdd8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11b35ab70>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11b3f6f60>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11b431cf8>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11af89f98>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11b497f60>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11afa6c50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11b4f4eb8>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11b240eb8>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11b537e48>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11b27bf60>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11b5e4828>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x11b776668>,\n",
       " <keras.engine.topology.Merge at 0x11b701128>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11bbb6ac8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11bbf3e48>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11b7fdcc0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11bc96400>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11b869c50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11bcf40b8>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11b8b8898>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11bab2e48>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11be83fd0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11c001940>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x11c162160>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11b78aa58>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11ba6fd68>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11bb39518>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11bed2f98>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11c0b3d68>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11c0e3c18>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11b7b9cf8>,\n",
       " <keras.engine.topology.Merge at 0x11bb9ed30>,\n",
       " <keras.engine.topology.Merge at 0x11c0ec4a8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11c189a58>,\n",
       " <keras.engine.topology.Merge at 0x11c1eb780>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11c6c2f98>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11c6dfc50>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11c2a1550>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11c87aeb8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11c420e80>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11c8b5f60>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11c4ced68>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11c575dd8>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11c91cdd8>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11cb30f60>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x11cba61d0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11c240e10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11c527ac8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11c600cf8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11c997b70>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11cb6bcf8>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x11cbf9fd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11c27cf98>,\n",
       " <keras.engine.topology.Merge at 0x11c690390>,\n",
       " <keras.engine.topology.Merge at 0x119152f60>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11cbef828>,\n",
       " <keras.engine.topology.Merge at 0x11cd88e10>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers # all layers in model (in order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import GlobalAveragePooling2D, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "                                    rotation_range=180,\n",
    "                                    rescale = 1./255.,\n",
    "                                    shear_range = .2,\n",
    "                                    zoom_range = .2,\n",
    "                                    horizontal_flip = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(100, activation='relu')(x) # optimize 100 here\n",
    "predictions = Dense(20, activation = 'softmax', name='leaf_preds')(x) # 20 = number of classes\n",
    "\n",
    "full_model = Model(model.input, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GlobalAveragePooling2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, None, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_model.compile('rmsprop', 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_model.fit_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Inception model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4033 images belonging to 4 classes.\n",
      "Found 1725 images belonging to 4 classes.\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "\n",
    "def jpg_counts(dirpath):\n",
    "    from os import listdir, walk\n",
    "    #from os.path import isfile, join\n",
    "    \n",
    "    # list of all subdirectories\n",
    "    dirlist = [x[0] for x in walk(dirpath)][1:]\n",
    "    \n",
    "    # list of all images in this directory\n",
    "    imagelist = [f for f in listdir(dirpath) if '.jpg' in f[-4:].lower()]\n",
    "    \n",
    "    # get all images in all subdirectories\n",
    "    #print(dirlist)\n",
    "    for currdir in dirlist:\n",
    "        imagelist += [f for f in listdir(currdir) if '.jpg' in f[-4:].lower()]\n",
    "    \n",
    "    return len(imagelist)\n",
    "\n",
    "def get_num_classes(dirpath, verbose=False):\n",
    "    from os import walk\n",
    "    \n",
    "    # get list of all direct subdirectories\n",
    "    dirlist = next(walk(dirpath))[1]\n",
    "    \n",
    "    if verbose:\n",
    "        print('Classes found:')\n",
    "        for d in dirlist:\n",
    "            print(d)\n",
    "    \n",
    "    return len(dirlist)\n",
    "\n",
    "'''\n",
    "basedir = '/Users/ggarbagnati/ds/metis/metisgh/sf17_ds5/local/Projects/05-Kojak'\n",
    "train_data_dir = basedir + '/data/train'\n",
    "validation_data_dir = basedir + '/data/validation'\n",
    "'''\n",
    "basedir = '/data/data'\n",
    "targetdir = basedir + '/train'\n",
    "valdir = basedir + '/validation'\n",
    "\n",
    "\n",
    "img_width, img_height = 761, 800\n",
    "nb_train_samples = jpg_counts(targetdir)\n",
    "nb_validation_samples = jpg_counts(valdir)\n",
    "nb_categories = get_num_classes(targetdir)\n",
    "nb_epoch = 1\n",
    "\n",
    "# create the base pre-trained model\n",
    "#base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "#x = base_model.output\n",
    "x = model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# add a fully-connected layer\n",
    "#x = Dense(1024, activation='relu', name='fc_1')(x)\n",
    "x = Dense(100, activation='relu', name='fc_1')(x)\n",
    "predictions = Dense(nb_categories, activation='softmax')(x)\n",
    "\n",
    "fullmodel = Model(input=model.input, output=predictions)\n",
    "\n",
    "# Freeze convolutional layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "#fullmodel.compile(optimizer=RMSprop(lr = .00001), loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "fullmodel.compile('rmsprop', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "train_datagen = ImageDataGenerator(rotation_range=180,\n",
    "                                    rescale = 1./255.,\n",
    "                                    shear_range = .2,\n",
    "                                    zoom_range = .2,\n",
    "                                    horizontal_flip = True)\n",
    "\n",
    "# Inception has a custom image preprocess function\n",
    "test_datagen = image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "generator_train = train_datagen.flow_from_directory(\n",
    "        targetdir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "generator_test = test_datagen.flow_from_directory(\n",
    "        valdir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "modelfull.fit_generator(generator_train,\n",
    "            samples_per_epoch = nb_train_samples,\n",
    "            nb_epoch = nb_epoch,\n",
    "            validation_data = generator_test,\n",
    "            nb_val_samples = nb_validation_samples)\n",
    "\n",
    "#start fine-tuning\n",
    "# unfreeze the top 2 inception blocks\n",
    "for layer in fullmodel.layers[:172]:\n",
    "   layer.trainable = False\n",
    "for layer in fullmodel.layers[172:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "# use SGD with a low learning rate\n",
    "fullmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9),\n",
    "            loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the top 2 inception blocks\n",
    "fullmodel.fit_generator(generator_train,\n",
    "                        samples_per_epoch = nb_train_samples,\n",
    "                        nb_epoch = nb_epoch,\n",
    "                        validation_data = generator_test,\n",
    "                        nb_val_samples = nb_validation_samples)\n",
    "\n",
    "model_json = fullmodel.to_json()\n",
    "with open('incep_3_multi.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "fullmodel.save_weights('incep_3_multi.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting from sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jpg_counts(dirpath, verbose=False):\n",
    "    from os import listdir, walk\n",
    "    #from os.path import isfile, join\n",
    "    \n",
    "    # list of all subdirectories\n",
    "    dirlist = [x[0] for x in walk(dirpath)][1:]\n",
    "    \n",
    "    # list of all images in this directory\n",
    "    imagelist = [f for f in listdir(dirpath) if '.jpg' in f[-4:].lower()]\n",
    "    if verbose:\n",
    "        print(len(imagelist),'\\n')\n",
    "    \n",
    "    # get all images in all subdirectories\n",
    "    #print(dirlist)\n",
    "    for currdir in dirlist:\n",
    "        allfiles = [f for f in listdir(currdir)]\n",
    "        imagelistsubdir = [f for f in listdir(currdir) if '.jpg' in f[-4:].lower()]\n",
    "        imagelist += imagelistsubdir\n",
    "        if verbose:\n",
    "            if len(allfiles) != len(imagelistsubdir):\n",
    "                print(currdir, len(imagelistsubdir), 'out of', len(allfiles), 'EXTRA NON JPG FILES')\n",
    "            else:\n",
    "                print(currdir, len(imagelistsubdir), 'out of', len(allfiles))\n",
    "\n",
    "    if verbose:\n",
    "        print(len(imagelist))\n",
    "    return len(imagelist)\n",
    "\n",
    "def get_num_classes(dirpath, verbose=False):\n",
    "    from os import walk\n",
    "    \n",
    "    # get list of all direct subdirectories\n",
    "    dirlist = next(walk(dirpath))[1]\n",
    "    \n",
    "    if verbose:\n",
    "        print('Classes found:')\n",
    "        for d in dirlist:\n",
    "            print(d)\n",
    "    \n",
    "    return len(dirlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'Soybean_test_1.JPG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categories: 46\n",
      "Top prediction for pred/Apple-Apple_scab-00008.JPG is: tomato_Leaf_Mold at 0.603696\n",
      "Top prediction for pred/Apple-healthy-01257.JPG is: Pepper,_bell_Bacterial_spot at 0.338878\n",
      "Top prediction for pred/Corn-healthy-00518.jpg is: banana_healthy at 0.438868\n",
      "Top prediction for pred/Corn-Northern_Leaf_Blight-02879.JPG is: soybean_Frogeye_leaf_spot at 0.439087\n",
      "Top prediction for pred/corn_northernleafblight_test_1.jpg is: apple_Black_rot at 0.998851\n",
      "Top prediction for pred/corn_northernleafblight_test_2.jpg is: Pepper,_bell_healthy at 0.891833\n",
      "Top prediction for pred/soybean_test_1.JPG is: soybean_Septoria_Leaf_Blight at 0.896122\n",
      "Top prediction for pred/Squash-Powdery_mildew-00006.JPG is: strawberry_Leaf_scorch at 0.568302\n",
      "Top prediction for pred/Strawberry-healthy-01120.JPG is: tomato_Target_Spot at 0.418308\n",
      "Top prediction for pred/Strawberry-Leaf_scorch-00009.JPG is: squash_healthy at 0.817026\n",
      "Top prediction for pred/Tomato-healthy-00458.JPG is: apple_Apple_scab at 0.715724\n",
      "Top prediction for pred/Tomato-Leaf_Mold-05888.JPG is: strawberry_Leaf_scorch at 0.806727\n",
      "Top prediction for pred/tomato-septoria_test_1.jpg is: potato_healthy at 0.515194\n",
      "Top prediction for pred/tomato-septoria_test_2.jpg is: squash_Powdery_mildew at 0.999728\n",
      "Top prediction for pred/tomato_test_1.jpg is: Pepper,_bell_Bacterial_spot at 0.392664\n",
      "Top prediction for pred/tomato_test_2.jpg is: strawberry_Leaf_scorch at 0.554846\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "#from keras.backend import image_data_format\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def scale_images(dirpath, newsize = (299,299)):\n",
    "    import numpy as np\n",
    "    from scipy.misc import imresize, imread\n",
    "    from os import listdir\n",
    "    \n",
    "    imagelist = [dirpath + '/' + f for f in listdir(dirpath) if '.jpg' in f[-4:].lower()]\n",
    "    imgs = []\n",
    "    \n",
    "    imgs = [np.transpose(imresize(imread(img), newsize),\n",
    "                        (0, 1, 2)).astype('float32')[:,:,:3] \n",
    "            for img in imagelist]\n",
    "    # the (0,1,2) might not be necessary?\n",
    "    # the [:,:,:3] is to prevent alpha channels from ruining the party\n",
    "    \n",
    "    return (np.array(imgs) / 255) , imagelist\n",
    "\n",
    "def json_to_classes_list(dirpath, verbose=False):\n",
    "    import json\n",
    "    from pprint import pprint\n",
    "\n",
    "    with open(dirpath) as json_data:\n",
    "        classes = json.load(json_data)\n",
    "\n",
    "\n",
    "    classes = {int(x):y for x,y in classes.items()}\n",
    "\n",
    "    if verbose:\n",
    "        pprint(classes)\n",
    "        \n",
    "    return classes\n",
    "\n",
    "'''\n",
    "img = load_img(url)  # this is a PIL image\n",
    "x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "'''\n",
    "\n",
    "input_shape = (150, 150, 3)\n",
    "num_categories = get_num_classes(targetdir)\n",
    "print('Number of categories:', num_categories)\n",
    "img_width, img_height = 150, 150\n",
    "#classes = next(walk(targetdir))[1]\n",
    "classes = json_to_classes_list('classeslist.json')\n",
    "\n",
    "\n",
    "basedir = 'data'\n",
    "#basedir = '/data/data2'\n",
    "targetdir = basedir + '/train'\n",
    "valdir = basedir + '/validation'\n",
    "preddir = 'pred'\n",
    "\n",
    "\n",
    "\n",
    "# IMAGE PROCESSING\n",
    "\n",
    "'''\n",
    "pred_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "pred_generator = pred_datagen.flow_from_directory(\n",
    "        preddir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "'''\n",
    "X, imglist = scale_images(preddir, newsize = (150,150))\n",
    "\n",
    "\n",
    "# MODEL ARCHITECTURE\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(Conv2D(32, (3, 3), input_shape=(3, 150, 150)))\n",
    "model.add(Conv2D(32, 3, 3, input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Conv2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Conv2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(Dense(1))\n",
    "model.add(Dense(num_categories)) # number of categories\n",
    "#model.add(Activation('sigmoid'))\n",
    "model.add(Activation('softmax')) # for multiclass\n",
    "\n",
    "model.load_weights('vgg16-1.h5')\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "'''\n",
    "model = Model(input= (base_model.input), output= (top_model(base_model.output)))\n",
    "'''\n",
    "\n",
    "preds = model.predict(X)\n",
    "\n",
    "for i, pred in enumerate(preds):\n",
    "    j = pred.argmax()\n",
    "    print('Top prediction for', imglist[i], 'is:', classes[j], 'at', pred[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_images(dirpath, newsize = (299,299)):\n",
    "    import numpy as np\n",
    "    from scipy.misc import imresize, imread\n",
    "    from os import listdir\n",
    "    \n",
    "    imagelist = [dirpath + '/' + f for f in listdir(dirpath) if '.jpg' in f[-4:].lower()]\n",
    "    imgs = []\n",
    "    \n",
    "    '''\n",
    "    for img in imagelist:\n",
    "        #print(img)\n",
    "        x = imread(img)\n",
    "        #print(x.shape)\n",
    "        x = imresize(x, newsize)\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        x = np.transpose(x, (0,1,2)).astype('float32')\n",
    "        #print(x.shape)\n",
    "        x = x[:,:,:3]\n",
    "        imgs.append(x)\n",
    "    '''\n",
    "    ''' Doesn't work with images that have an alpha channel\n",
    "    imgs = [np.transpose(imresize(imread(img), newsize),\n",
    "                        (0, 1, 2)).astype('float32') \n",
    "            for img in imagelist] # the (0,1,2) might not be necessary?\n",
    "    '''\n",
    "    \n",
    "    imgs = [np.transpose(imresize(imread(img), newsize),\n",
    "                        (0, 1, 2)).astype('float32')[:,:,:3] \n",
    "            for img in imagelist]\n",
    "    # the (0,1,2) might not be necessary?\n",
    "    # the [:,:,:3] is to prevent alpha channels from ruining the party\n",
    "    \n",
    "    return (np.array(imgs) / 255) , imagelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preddir = 'pred'\n",
    "newsize = (150,150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 150, 150, 3)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_images(preddir,newsize=newsize)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = image.load_img(img_path, target_size=(299, 299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting from InceptionV3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categories: 46\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nmodel = Model(input= (base_model.input), output= (top_model(base_model.output)))\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.models import load_model\n",
    "\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Conv2D, MaxPooling2D\n",
    "# from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "# from keras import backend as K\n",
    "#from keras.backend import image_data_format\n",
    "\n",
    "\n",
    "def scale_images(dirpath, newsize = (299,299)):\n",
    "    import numpy as np\n",
    "    from scipy.misc import imresize, imread\n",
    "    from os import listdir\n",
    "    \n",
    "    imagelist = [dirpath + '/' + f for f in listdir(dirpath) if '.jpg' in f[-4:].lower()]\n",
    "    \n",
    "    imgs = [imresize(imread(img), newsize).astype('float32')[:,:,:3] \n",
    "            for img in imagelist]\n",
    "    # the [:,:,:3] is to prevent alpha channels from ruining the party\n",
    "    \n",
    "    return (np.array(imgs) / 255) , imagelist\n",
    "\n",
    "def json_to_classes_list(dirpath, verbose=False):\n",
    "    import json\n",
    "    from pprint import pprint\n",
    "\n",
    "    with open(dirpath) as json_data:\n",
    "        classes = json.load(json_data)\n",
    "\n",
    "\n",
    "    classes = {int(x):y for x,y in classes.items()}\n",
    "\n",
    "    if verbose:\n",
    "        pprint(classes)\n",
    "        \n",
    "    return classes\n",
    "\n",
    "'''\n",
    "img = load_img(url)  # this is a PIL image\n",
    "x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "'''\n",
    "\n",
    "input_shape = (299, 299, 3)\n",
    "num_categories = get_num_classes(targetdir)\n",
    "print('Number of categories:', num_categories)\n",
    "img_width, img_height = 299, 299\n",
    "#classes = next(walk(targetdir))[1]\n",
    "classes = json_to_classes_list('classeslist.json')\n",
    "\n",
    "\n",
    "basedir = 'data'\n",
    "#basedir = '/data/data2'\n",
    "targetdir = basedir + '/train'\n",
    "valdir = basedir + '/validation'\n",
    "preddir = 'pred'\n",
    "\n",
    "\n",
    "\n",
    "# IMAGE PROCESSING\n",
    "\n",
    "X, imglist = scale_images(preddir)\n",
    "\n",
    "# LOADING MODEL + MODEL ARCHITECTURE\n",
    "\n",
    "modelloc = 'leafincepmodel-ft-3.h5'\n",
    "\n",
    "model = load_model(modelloc)\n",
    "\n",
    "'''\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "'''\n",
    "\n",
    "'''\n",
    "model = Model(input= (base_model.input), output= (top_model(base_model.output)))\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# for i, pred in enumerate(preds):\n",
    "#     top3 = pred.argsort()[::-1][:3]\n",
    "#     #j = pred.argmax()\n",
    "#     print('Top prediction/s for', imglist[i], ':')\n",
    "#     print('\\t(1)', classes[top3[0]], 'at', pred[top3[0]])\n",
    "#     print('\\t(2)', classes[top3[1]], 'at', pred[top3[1]])\n",
    "#     print('\\t(3)', classes[top3[2]], 'at', pred[top3[2]])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from keras.utils import np_utils\n",
    "y_proba = model.predict(X)\n",
    "y_classes = np_utils.probas_to_classes(y_proba)\n",
    "y_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict plant (look for first \"_\") in prediction, set threshold to 70%?\n",
    "Predict plant health/disease (output top three in that plant category if above predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top prediction/s for pred/Apple-Black_rot-00639.JPG :\n",
      "\t(1) apple_Black_rot at 0.988216\n",
      "\t(2) peach_Bacterial_spot at 0.0099152\n",
      "\t(3) apple_Apple_scab at 0.00170946\n",
      "\n",
      "Top prediction/s for pred/Apple-Black_rot-00673.JPG :\n",
      "\t(1) apple_Black_rot at 0.559321\n",
      "\t(2) apple_Apple_scab at 0.275908\n",
      "\t(3) squash_Powdery_mildew at 0.0874202\n",
      "\n",
      "Top prediction/s for pred/Banana-Banana_speckle-05113.JPG :\n",
      "\t(1) banana_Banana_speckle at 0.598356\n",
      "\t(2) banana_healthy at 0.401183\n",
      "\t(3) banana_Black_sigatoka_(Black_leaf_streak) at 0.000458387\n",
      "\n",
      "Top prediction/s for pred/Banana-Banana_speckle-05134.JPG :\n",
      "\t(1) banana_Banana_speckle at 0.974927\n",
      "\t(2) banana_healthy at 0.0249178\n",
      "\t(3) banana_Black_sigatoka_(Black_leaf_streak) at 0.00013229\n",
      "\n",
      "Top prediction/s for pred/Banana-Banana_speckle-05151.JPG :\n",
      "\t(1) banana_Banana_speckle at 0.999971\n",
      "\t(2) banana_Black_sigatoka_(Black_leaf_streak) at 2.83259e-05\n",
      "\t(3) banana_healthy at 4.23629e-07\n",
      "\n",
      "Top prediction/s for pred/Cabbage-Black_rot-00024.JPG :\n",
      "\t(1) cabbage_Black_rot at 0.999107\n",
      "\t(2) soybean_Frogeye_leaf_spot at 0.000477293\n",
      "\t(3) soybean_healthy at 0.00033182\n",
      "\n",
      "Top prediction/s for pred/Corn-Common_rust-01702.JPG :\n",
      "\t(1) corn_Common_rust at 1.0\n",
      "\t(2) tomato_Septoria_leaf_spot at 1.09574e-07\n",
      "\t(3) Pepper,_bell_Bacterial_spot at 4.6547e-08\n",
      "\n",
      "Top prediction/s for pred/Cucumber-Downy_mildew-00037.JPG :\n",
      "\t(1) cucumber_Downy_mildew at 0.999765\n",
      "\t(2) cabbage_Black_rot at 0.000143632\n",
      "\t(3) cucumber_healthy at 4.47507e-05\n",
      "\n",
      "Top prediction/s for pred/Cucumber-healthy-01345.JPG :\n",
      "\t(1) cucumber_healthy at 0.742289\n",
      "\t(2) squash_healthy at 0.231408\n",
      "\t(3) cucumber_Downy_mildew at 0.0262575\n",
      "\n",
      "Top prediction/s for pred/Grape-healthy-04063.jpg :\n",
      "\t(1) grape_healthy at 0.999502\n",
      "\t(2) banana_healthy at 0.000244548\n",
      "\t(3) apple_healthy at 6.00838e-05\n",
      "\n",
      "Top prediction/s for pred/Grape-healthy-04147.JPG :\n",
      "\t(1) grape_healthy at 0.998978\n",
      "\t(2) apple_Black_rot at 0.000479985\n",
      "\t(3) apple_healthy at 0.000395325\n",
      "\n",
      "Top prediction/s for pred/Soybean-healthy-09764.JPG :\n",
      "\t(1) soybean_healthy at 0.744173\n",
      "\t(2) soybean_Frogeye_leaf_spot at 0.255555\n",
      "\t(3) soybean_Septoria_Leaf_Blight at 0.000251749\n",
      "\n",
      "Top prediction/s for pred/Soybean-healthy-09781.JPG :\n",
      "\t(1) soybean_healthy at 0.982355\n",
      "\t(2) soybean_Frogeye_leaf_spot at 0.0136644\n",
      "\t(3) soybean_Downy_mildew at 0.00390304\n",
      "\n",
      "Top prediction/s for pred/Soybean-Septoria_Leaf_Blight-05129.JPG :\n",
      "\t(1) soybean_Septoria_Leaf_Blight at 0.999724\n",
      "\t(2) tomato_Early_blight at 0.000245306\n",
      "\t(3) potato_Early_blight at 2.9543e-05\n",
      "\n",
      "Top prediction/s for pred/Strawberry-healthy-01282.JPG :\n",
      "\t(1) strawberry_healthy at 0.999998\n",
      "\t(2) strawberry_Leaf_scorch at 1.60822e-06\n",
      "\t(3) tomato_Late_blight at 1.94541e-07\n",
      "\n",
      "Top prediction/s for pred/Strawberry-Leaf_scorch-00051.JPG :\n",
      "\t(1) strawberry_Leaf_scorch at 0.999986\n",
      "\t(2) grape_Esca_(Black_Measles_or_Spanish_Measles) at 1.23321e-05\n",
      "\t(3) tomato_Septoria_leaf_spot at 4.94293e-07\n",
      "\n",
      "Top prediction/s for pred/Tomato-Bacterial_spot-01620.JPG :\n",
      "\t(1) tomato_Late_blight at 0.414814\n",
      "\t(2) tomato_Tomato_Yellow_Leaf_Curl_disease at 0.198143\n",
      "\t(3) tomato_Bacterial_spot at 0.197168\n",
      "\n",
      "Top prediction/s for pred/Tomato-Early_blight-01994.JPG :\n",
      "\t(1) tomato_Early_blight at 0.996854\n",
      "\t(2) tomato_Septoria_leaf_spot at 0.00231106\n",
      "\t(3) tomato_Bacterial_spot at 0.000820346\n",
      "\n",
      "Top prediction/s for pred/Tomato-healthy-00516.JPG :\n",
      "\t(1) tomato_healthy at 0.999565\n",
      "\t(2) tomato_Late_blight at 0.000405242\n",
      "\t(3) tomato_Spider_mites??Two-spotted_spider_mite at 1.16074e-05\n",
      "\n",
      "Top prediction/s for pred/Tomato-Target_Spot-12994.JPG :\n",
      "\t(1) tomato_Target_Spot at 0.900321\n",
      "\t(2) tomato_Bacterial_spot at 0.0888752\n",
      "\t(3) tomato_Early_blight at 0.00441635\n",
      "\n",
      "Top prediction/s for pred/Tomato-Target_Spot-13002.JPG :\n",
      "\t(1) tomato_Tomato_Yellow_Leaf_Curl_disease at 0.432145\n",
      "\t(2) tomato_Bacterial_spot at 0.415599\n",
      "\t(3) tomato_Late_blight at 0.0552656\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, imglist = scale_images(preddir)\n",
    "preds = model.predict(X)\n",
    "for i, pred in enumerate(preds):\n",
    "    top3 = pred.argsort()[::-1][:3]\n",
    "    #j = pred.argmax()\n",
    "    print('Top prediction/s for', imglist[i], ':')\n",
    "    print('\\t(1)', classes[top3[0]], 'at', pred[top3[0]])\n",
    "    print('\\t(2)', classes[top3[1]], 'at', pred[top3[1]])\n",
    "    print('\\t(3)', classes[top3[2]], 'at', pred[top3[2]])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top prediction/s for pred/Tomato-Bacterial_spot-01592.JPG :\n",
      "\t(1) tomato_Bacterial_spot at 0.987302\n",
      "\t(2) tomato_Septoria_leaf_spot at 0.00839303\n",
      "\t(3) tomato_Early_blight at 0.00406035\n",
      "\n",
      "Top prediction/s for pred/Tomato-Bacterial_spot-01593.JPG :\n",
      "\t(1) tomato_Bacterial_spot at 0.939456\n",
      "\t(2) tomato_Early_blight at 0.0518083\n",
      "\t(3) tomato_Septoria_leaf_spot at 0.00463675\n",
      "\n",
      "Top prediction/s for pred/Tomato-Bacterial_spot-01594.JPG :\n",
      "\t(1) tomato_Bacterial_spot at 0.990683\n",
      "\t(2) tomato_Septoria_leaf_spot at 0.00736429\n",
      "\t(3) tomato_Tomato_Yellow_Leaf_Curl_disease at 0.00118015\n",
      "\n",
      "Top prediction/s for pred/Tomato-Bacterial_spot-01597.JPG :\n",
      "\t(1) tomato_Tomato_Yellow_Leaf_Curl_disease at 0.449069\n",
      "\t(2) tomato_Late_blight at 0.374783\n",
      "\t(3) tomato_Bacterial_spot at 0.144853\n",
      "\n",
      "Top prediction/s for pred/Tomato-Bacterial_spot-01598.JPG :\n",
      "\t(1) tomato_Tomato_Yellow_Leaf_Curl_disease at 0.864269\n",
      "\t(2) tomato_Late_blight at 0.0678573\n",
      "\t(3) tomato_Bacterial_spot at 0.0665755\n",
      "\n",
      "Top prediction/s for pred/Tomato-Bacterial_spot-01602.JPG :\n",
      "\t(1) tomato_Bacterial_spot at 0.907029\n",
      "\t(2) tomato_Septoria_leaf_spot at 0.057427\n",
      "\t(3) tomato_Early_blight at 0.0224042\n",
      "\n",
      "Top prediction/s for pred/Tomato-Bacterial_spot-01604.JPG :\n",
      "\t(1) tomato_Bacterial_spot at 0.976242\n",
      "\t(2) tomato_Late_blight at 0.0228069\n",
      "\t(3) tomato_Early_blight at 0.000591256\n",
      "\n",
      "Top prediction/s for pred/Tomato-Bacterial_spot-01609.JPG :\n",
      "\t(1) tomato_Bacterial_spot at 0.99846\n",
      "\t(2) tomato_Septoria_leaf_spot at 0.00113599\n",
      "\t(3) tomato_Tomato_Yellow_Leaf_Curl_disease at 0.000217426\n",
      "\n",
      "Top prediction/s for pred/Tomato-Bacterial_spot-01615.JPG :\n",
      "\t(1) tomato_Bacterial_spot at 0.803839\n",
      "\t(2) tomato_Tomato_Yellow_Leaf_Curl_disease at 0.181346\n",
      "\t(3) tomato_Late_blight at 0.00815868\n",
      "\n",
      "Top prediction/s for pred/Tomato-Bacterial_spot-01619.JPG :\n",
      "\t(1) tomato_Early_blight at 0.525695\n",
      "\t(2) tomato_Tomato_Yellow_Leaf_Curl_disease at 0.277999\n",
      "\t(3) tomato_Bacterial_spot at 0.16044\n",
      "\n",
      "Top prediction/s for pred/Tomato-Bacterial_spot-01620.JPG :\n",
      "\t(1) tomato_Late_blight at 0.414814\n",
      "\t(2) tomato_Tomato_Yellow_Leaf_Curl_disease at 0.198143\n",
      "\t(3) tomato_Bacterial_spot at 0.197168\n",
      "\n",
      "Top prediction/s for pred/Tomato-Bacterial_spot-01628.JPG :\n",
      "\t(1) tomato_Bacterial_spot at 0.995912\n",
      "\t(2) tomato_Early_blight at 0.00377284\n",
      "\t(3) tomato_Septoria_leaf_spot at 0.000295141\n",
      "\n",
      "Top prediction/s for pred/Tomato-Bacterial_spot-01629.JPG :\n",
      "\t(1) tomato_Bacterial_spot at 0.681835\n",
      "\t(2) tomato_Early_blight at 0.191405\n",
      "\t(3) tomato_Late_blight at 0.0613795\n",
      "\n",
      "Top prediction/s for pred/Tomato-Bacterial_spot-01630.JPG :\n",
      "\t(1) tomato_Bacterial_spot at 0.996833\n",
      "\t(2) tomato_Early_blight at 0.00245635\n",
      "\t(3) strawberry_Leaf_scorch at 0.000565228\n",
      "\n",
      "Top prediction/s for pred/Tomato-Bacterial_spot-01637.JPG :\n",
      "\t(1) tomato_Bacterial_spot at 0.591254\n",
      "\t(2) tomato_Tomato_Yellow_Leaf_Curl_disease at 0.322841\n",
      "\t(3) tomato_Late_blight at 0.0821671\n",
      "\n",
      "Top prediction/s for pred/Tomato-Bacterial_spot-01641.JPG :\n",
      "\t(1) tomato_Tomato_Yellow_Leaf_Curl_disease at 0.526952\n",
      "\t(2) tomato_Early_blight at 0.320658\n",
      "\t(3) tomato_Bacterial_spot at 0.0692485\n",
      "\n",
      "Top prediction/s for pred/Tomato-Bacterial_spot-01643.JPG :\n",
      "\t(1) tomato_Bacterial_spot at 0.999281\n",
      "\t(2) tomato_Early_blight at 0.000223972\n",
      "\t(3) tomato_Septoria_leaf_spot at 0.000191324\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, imglist = scale_images(preddir)\n",
    "preds = model.predict(X)\n",
    "for i, pred in enumerate(preds):\n",
    "    top3 = pred.argsort()[::-1][:3]\n",
    "    #j = pred.argmax()\n",
    "    print('Top prediction/s for', imglist[i], ':')\n",
    "    print('\\t(1)', classes[top3[0]], 'at', pred[top3[0]])\n",
    "    print('\\t(2)', classes[top3[1]], 'at', pred[top3[1]])\n",
    "    print('\\t(3)', classes[top3[2]], 'at', pred[top3[2]])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 299, 299, 3)\n",
      "Top prediction/s for pred/Apple-01.JPG :\n",
      "\t(1) apple_healthy at 0.998036\n",
      "\t(2) apple_Black_rot at 0.00186758\n",
      "\t(3) grape_healthy at 5.92782e-05\n",
      "\n",
      "Top prediction/s for pred/Grape-01.JPG :\n",
      "\t(1) grape_healthy at 0.650656\n",
      "\t(2) apple_healthy at 0.331087\n",
      "\t(3) apple_Black_rot at 0.0151621\n",
      "\n",
      "Top prediction/s for pred/Strawberry-01.JPG :\n",
      "\t(1) strawberry_healthy at 0.999998\n",
      "\t(2) strawberry_Leaf_scorch at 1.60822e-06\n",
      "\t(3) tomato_Late_blight at 1.94541e-07\n",
      "\n",
      "Top prediction/s for pred/Strawberry-02.JPG :\n",
      "\t(1) strawberry_Leaf_scorch at 0.999986\n",
      "\t(2) grape_Esca_(Black_Measles_or_Spanish_Measles) at 1.23321e-05\n",
      "\t(3) tomato_Septoria_leaf_spot at 4.94293e-07\n",
      "\n",
      "Top prediction/s for pred/Strawberry-03.JPG :\n",
      "\t(1) strawberry_Leaf_scorch at 0.985764\n",
      "\t(2) tomato_Tomato_Yellow_Leaf_Curl_disease at 0.0126932\n",
      "\t(3) potato_Early_blight at 0.000900258\n",
      "\n",
      "Top prediction/s for pred/Strawberry-04.JPG :\n",
      "\t(1) strawberry_Leaf_scorch at 0.999787\n",
      "\t(2) tomato_Early_blight at 0.00013004\n",
      "\t(3) tomato_Septoria_leaf_spot at 7.50815e-05\n",
      "\n",
      "Top prediction/s for pred/Tomato-Bacterial_spot-01593.JPG :\n",
      "\t(1) tomato_Bacterial_spot at 0.939456\n",
      "\t(2) tomato_Early_blight at 0.0518083\n",
      "\t(3) tomato_Septoria_leaf_spot at 0.00463675\n",
      "\n",
      "Top prediction/s for pred/Tomato-Bacterial_spot-01641.JPG :\n",
      "\t(1) tomato_Tomato_Yellow_Leaf_Curl_disease at 0.526952\n",
      "\t(2) tomato_Early_blight at 0.320658\n",
      "\t(3) tomato_Bacterial_spot at 0.0692485\n",
      "\n",
      "Top prediction/s for pred/Tomato-Tomato_Yellow_Leaf_Curl_disease-08510.JPG :\n",
      "\t(1) tomato_Tomato_Yellow_Leaf_Curl_disease at 0.999983\n",
      "\t(2) tomato_Spider_mites??Two-spotted_spider_mite at 8.78081e-06\n",
      "\t(3) Pepper,_bell_Bacterial_spot at 3.3733e-06\n",
      "\n",
      "Top prediction/s for pred/Tomato-Tomato_Yellow_Leaf_Curl_disease-08514.JPG :\n",
      "\t(1) tomato_Tomato_Yellow_Leaf_Curl_disease at 0.999993\n",
      "\t(2) tomato_Spider_mites??Two-spotted_spider_mite at 5.16522e-06\n",
      "\t(3) tomato_Leaf_Mold at 4.19428e-07\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, imglist = scale_images(preddir)\n",
    "preds = model.predict(X)\n",
    "for i, pred in enumerate(preds):\n",
    "    top3 = pred.argsort()[::-1][:3]\n",
    "    #j = pred.argmax()\n",
    "    print('Top prediction/s for', imglist[i], ':')\n",
    "    print('\\t(1)', classes[top3[0]], 'at', pred[top3[0]])\n",
    "    print('\\t(2)', classes[top3[1]], 'at', pred[top3[1]])\n",
    "    print('\\t(3)', classes[top3[2]], 'at', pred[top3[2]])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pepper,_bell_Bacterial_spot\n",
      "Pepper,_bell_healthy\n",
      "apple_Apple_scab\n",
      "apple_Black_rot\n",
      "apple_Cedar_apple_rust\n",
      "apple_healthy\n",
      "banana_Banana_speckle\n",
      "banana_Black_sigatoka_(Black_leaf_streak)\n",
      "banana_healthy\n",
      "cabbage_Black_rot\n",
      "cabbage_healthy\n",
      "cherry_Powdery_mildew\n",
      "cherry_healthy\n",
      "corn_Cercospora_leaf_spot_Gray_leaf_spot\n",
      "corn_Common_rust\n",
      "corn_Northern_Leaf_Blight\n",
      "corn_healthy\n",
      "cucumber_Downy_mildew\n",
      "cucumber_healthy\n",
      "grape_Black_rot\n",
      "grape_Esca_(Black_Measles_or_Spanish_Measles)\n",
      "grape_Leaf_blight_(Isariopsis_Leaf_Spot)\n",
      "grape_healthy\n",
      "peach_Bacterial_spot\n",
      "peach_healthy\n",
      "potato_Early_blight\n",
      "potato_Late_blight\n",
      "potato_healthy\n",
      "soybean_Downy_mildew\n",
      "soybean_Frogeye_leaf_spot\n",
      "soybean_Septoria_Leaf_Blight\n",
      "soybean_healthy\n",
      "squash_Powdery_mildew\n",
      "squash_healthy\n",
      "strawberry_Leaf_scorch\n",
      "strawberry_healthy\n",
      "tomato_Bacterial_spot\n",
      "tomato_Early_blight\n",
      "tomato_Late_blight\n",
      "tomato_Leaf_Mold\n",
      "tomato_Septoria_leaf_spot\n",
      "tomato_Spider_mites??Two-spotted_spider_mite\n",
      "tomato_Target_Spot\n",
      "tomato_Tomato_Yellow_Leaf_Curl_disease\n",
      "tomato_Tomato_mosaic_virus\n",
      "tomato_healthy\n"
     ]
    }
   ],
   "source": [
    "for i in classes:\n",
    "    print(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_plant_name(plantstring, replace_underscores=None, remove_parentheses=False):\n",
    "    \n",
    "    ## basic parsing\n",
    "    underscore_i = plantstring.find('_')\n",
    "    plantsp = plantstring[:underscore_i]\n",
    "    healthstatus = plantstring[underscore_i+1:]\n",
    "    \n",
    "    ## special cases\n",
    "    # special case for bell peppers (2 cases)\n",
    "    if plantsp == 'Pepper,':\n",
    "        plantsp = 'bell_pepper'\n",
    "        underscore_i = healthstatus.find('_')\n",
    "        healthstatus = healthstatus[underscore_i+1:]\n",
    "    \n",
    "    # special case for 'tomato_Spider_mites??Two-spotted_spider_mite' (1 case)\n",
    "    if healthstatus == 'Spider_mites??Two-spotted_spider_mite':\n",
    "        healthstatus = 'Spider_mites_(Two-spotted_spider_mite)'\n",
    "    \n",
    "    ## extra parameters\n",
    "    if replace_underscores != None:\n",
    "        plantsp = plantsp.replace('_', replace_underscores)\n",
    "        healthstatus = healthstatus.replace('_', replace_underscores)\n",
    "        \n",
    "    if remove_parentheses:\n",
    "        plantsp = plantsp.replace('(','')\n",
    "        plantsp = plantsp.replace(')','')\n",
    "        healthstatus = healthstatus.replace('(','')\n",
    "        healthstatus = healthstatus.replace(')','')\n",
    "    \n",
    "    return plantsp, healthstatus\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tomato', 'Spider_mites_Two-spotted_spider_mite')"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_plant_name('tomato_Spider_mites??Two-spotted_spider_mite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('bell_pepper', 'Bacterial_spot')\n",
      "('bell_pepper', 'healthy')\n",
      "('apple', 'Apple_scab')\n",
      "('apple', 'Black_rot')\n",
      "('apple', 'Cedar_apple_rust')\n",
      "('apple', 'healthy')\n",
      "('banana', 'Banana_speckle')\n",
      "('banana', 'Black_sigatoka_(Black_leaf_streak)')\n",
      "('banana', 'healthy')\n",
      "('cabbage', 'Black_rot')\n",
      "('cabbage', 'healthy')\n",
      "('cherry', 'Powdery_mildew')\n",
      "('cherry', 'healthy')\n",
      "('corn', 'Cercospora_leaf_spot_Gray_leaf_spot')\n",
      "('corn', 'Common_rust')\n",
      "('corn', 'Northern_Leaf_Blight')\n",
      "('corn', 'healthy')\n",
      "('cucumber', 'Downy_mildew')\n",
      "('cucumber', 'healthy')\n",
      "('grape', 'Black_rot')\n",
      "('grape', 'Esca_(Black_Measles_or_Spanish_Measles)')\n",
      "('grape', 'Leaf_blight_(Isariopsis_Leaf_Spot)')\n",
      "('grape', 'healthy')\n",
      "('peach', 'Bacterial_spot')\n",
      "('peach', 'healthy')\n",
      "('potato', 'Early_blight')\n",
      "('potato', 'Late_blight')\n",
      "('potato', 'healthy')\n",
      "('soybean', 'Downy_mildew')\n",
      "('soybean', 'Frogeye_leaf_spot')\n",
      "('soybean', 'Septoria_Leaf_Blight')\n",
      "('soybean', 'healthy')\n",
      "('squash', 'Powdery_mildew')\n",
      "('squash', 'healthy')\n",
      "('strawberry', 'Leaf_scorch')\n",
      "('strawberry', 'healthy')\n",
      "('tomato', 'Bacterial_spot')\n",
      "('tomato', 'Early_blight')\n",
      "('tomato', 'Late_blight')\n",
      "('tomato', 'Leaf_Mold')\n",
      "('tomato', 'Septoria_leaf_spot')\n",
      "('tomato', 'Spider_mites_(Two-spotted_spider_mite)')\n",
      "('tomato', 'Target_Spot')\n",
      "('tomato', 'Tomato_Yellow_Leaf_Curl_disease')\n",
      "('tomato', 'Tomato_mosaic_virus')\n",
      "('tomato', 'healthy')\n"
     ]
    }
   ],
   "source": [
    "for i in classes:\n",
    "    print(parse_plant_name(classes[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python script for web app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (Unable to open file: name = 'leafincepmodel-ft-2.h5', errno = 2, error message = 'no such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-425f740f3b3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mmodelloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'leafincepmodel-ft-2.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m## IMAGE PROCESSING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ggarbagnati/anaconda/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;31m# instantiate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ggarbagnati/anaconda/lib/python3.5/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ggarbagnati/anaconda/lib/python3.5/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/Users/ilan/minonda/conda-bld/work/h5py/_objects.c:2696)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/Users/ilan/minonda/conda-bld/work/h5py/_objects.c:2654)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open (/Users/ilan/minonda/conda-bld/work/h5py/h5f.c:1942)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (Unable to open file: name = 'leafincepmodel-ft-2.h5', errno = 2, error message = 'no such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.models import load_model\n",
    "\n",
    "import numpy as np\n",
    "from scipy.misc import imresize, imread\n",
    "from os import listdir\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "## Helper fxns\n",
    "\n",
    "def scale_images(dirpath, newsize = (299,299)):\n",
    "    imagelist = [dirpath + '/' + f for f in listdir(dirpath) if '.jpg' in f[-4:].lower()]\n",
    "    \n",
    "    imgs = [imresize(imread(img), newsize).astype('float32')[:,:,:3] \n",
    "            for img in imagelist]\n",
    "    # the [:,:,:3] is to prevent alpha channels from ruining the party\n",
    "    \n",
    "    print(np.array(imgs).shape)\n",
    "    \n",
    "    \n",
    "    return (np.array(imgs) / 255) , imagelist\n",
    "\n",
    "def json_to_classes_list(dirpath, verbose=False):\n",
    "    with open(dirpath) as json_data:\n",
    "        classes = json.load(json_data)\n",
    "\n",
    "    classes = {int(x):y for x,y in classes.items()}\n",
    "\n",
    "    if verbose:\n",
    "        pprint(classes)\n",
    "        \n",
    "    return classes\n",
    "\n",
    "def parse_plant_name(plantstring, replace_underscores=None, remove_parentheses=False):\n",
    "    ## basic parsing\n",
    "    underscore_i = plantstring.find('_')\n",
    "    plantsp = plantstring[:underscore_i]\n",
    "    healthstatus = plantstring[underscore_i+1:]\n",
    "    \n",
    "    ## special cases\n",
    "    # special case for bell peppers (2 cases)\n",
    "    if plantsp == 'Pepper,':\n",
    "        plantsp = 'bell_pepper'\n",
    "        underscore_i = healthstatus.find('_')\n",
    "        healthstatus = healthstatus[underscore_i+1:]\n",
    "    \n",
    "    # special case for 'tomato_Spider_mites??Two-spotted_spider_mite' (1 case)\n",
    "    if healthstatus == 'Spider_mites??Two-spotted_spider_mite':\n",
    "        healthstatus = 'Spider_mites_(Two-spotted_spider_mite)'\n",
    "    \n",
    "    ## extra parameters\n",
    "    if replace_underscores != None:\n",
    "        plantsp = plantsp.replace('_', replace_underscores)\n",
    "        healthstatus = healthstatus.replace('_', replace_underscores)\n",
    "        \n",
    "    if remove_parentheses:\n",
    "        plantsp = plantsp.replace('(','')\n",
    "        plantsp = plantsp.replace(')','')\n",
    "        healthstatus = healthstatus.replace('(','')\n",
    "        healthstatus = healthstatus.replace(')','')\n",
    "    \n",
    "    return plantsp, healthstatus\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## LOADING MODEL + MODEL ARCHITECTURE\n",
    "\n",
    "modelloc = 'leafincepmodel-ft-2.h5'\n",
    "\n",
    "model = load_model(modelloc)\n",
    "\n",
    "## IMAGE PROCESSING\n",
    "\n",
    "#input_shape = (299, 299, 3)\n",
    "img_width, img_height = 299, 299\n",
    "classes = json_to_classes_list('classeslist.json')\n",
    "preddir = 'pred'\n",
    "X, imglist = scale_images(preddir)\n",
    "\n",
    "## PREDICTIONS\n",
    "\n",
    "preds = model.predict(X)\n",
    "for i, pred in enumerate(preds):\n",
    "    top3 = pred.argsort()[::-1][:3]\n",
    "    print('Top prediction/s for', imglist[i], ':')\n",
    "    print('\\t(1)', classes[top3[0]], 'at', pred[top3[0]])\n",
    "    print('\\t(2)', classes[top3[1]], 'at', pred[top3[1]])\n",
    "    print('\\t(3)', classes[top3[2]], 'at', pred[top3[2]])\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "modelloc = '~/model/model.h5'\n",
    "\n",
    "model = load_model(modelloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "modelloc = 'leafincepmodel-ft-3.h5'\n",
    "\n",
    "fullmodel = load_model(modelloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basedir = '/Users/ggarbagnati/ds/metis/metisgh/sf17_ds5/local/Projects/05-Kojak'\n",
    "targetdir = basedir + '/data/train'\n",
    "valdir = basedir + '/data/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22099 images belonging to 46 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "img_width, img_height = 299, 299 # inception likes 299x299\n",
    "batch_size = 32\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "generator_test = test_datagen.flow_from_directory(\n",
    "        valdir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.028518684208393097, 1.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullmodel.evaluate_generator(generator_test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullmodel.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'99.23'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'{0:.2f}'.format(99.231341243)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
